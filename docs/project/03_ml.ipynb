{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bb1fbf-27f9-43bd-8d65-8c9a19f340bc",
   "metadata": {},
   "source": [
    "# ðŸ¤– Machine Learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "The Titanic dataset is a popular and classic dataset used for introducing machine learning concepts and techniques. This dataset contains information about the passengers aboard the Titanic, including features such as age, gender, ticket class, and whether or not they survived the disaster. The primary objective is to build a predictive model that can accurately classify whether a passenger survived or not based on these features.\n",
    "\n",
    "Machine learning offers a range of algorithms that can be applied to this classification problem. These algorithms can be broadly categorized into supervised learning techniques, where the model is trained on a labeled dataset. For the Titanic dataset, this means using the known outcomes (survived or not) to train the model.\n",
    "\n",
    "Key steps in applying machine learning to the Titanic dataset include:\n",
    "\n",
    "1. **Data Preprocessing:** This involves cleaning the data, handling missing values, and performing feature engineering to create relevant features that will improve the model's performance. The preprocessing steps ensure that the data is in a suitable format for training.\n",
    "\n",
    "2. **Splitting the Data:** The dataset is typically split into a training set and a test set. The training set is used to train the model, while the test set is used to evaluate its performance.\n",
    "\n",
    "3. **Selecting and Training Models:** Various machine learning algorithms can be applied to the Titanic dataset, including:\n",
    "   - **Logistic Regression:** A simple and interpretable algorithm suitable for binary classification problems.\n",
    "   - **Decision Trees:** A non-linear model that captures complex interactions between features.\n",
    "   - **Random Forests:** An ensemble method that builds multiple decision trees and combines their predictions for improved accuracy.\n",
    "   - **Support Vector Machines (SVM):** A powerful classifier that can find the optimal boundary between classes.\n",
    "   - **Gradient Boosting:** An ensemble technique that builds models sequentially to correct errors made by previous models.\n",
    "\n",
    "4. **Model Evaluation:** The performance of the models is evaluated using metrics such as accuracy, precision, recall, and the F1 score. Cross-validation techniques can also be employed to ensure the model's robustness and to prevent overfitting.\n",
    "\n",
    "5. **Hyperparameter Tuning:** This involves optimizing the parameters of the chosen algorithms to improve their performance. Techniques like grid search or random search can be used for this purpose.\n",
    "\n",
    "6. **Making Predictions:** Once the model is trained and evaluated, it can be used to make predictions on new, unseen data. In the case of the Titanic dataset, this would involve predicting the survival of passengers based on their features.\n",
    "\n",
    "By applying machine learning techniques to the Titanic dataset, we can gain valuable insights into the factors that influenced survival and develop predictive models that can be used for similar classification tasks in other domains. The process also provides a practical introduction to key machine learning concepts and methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0524899-25ed-4bff-8644-8e02a328ba25",
   "metadata": {},
   "source": [
    "## Apply Machine Learning Models\n",
    "\n",
    "In this section, we will apply various machine learning models to the Titanic dataset to predict passenger survival. By leveraging algorithms such as Logistic Regression, Decision Trees, Random Forests, Support Vector Machines (SVM), and Gradient Boosting, we aim to build and evaluate predictive models. These models will help us understand the key factors influencing survival and demonstrate the application of machine learning techniques to real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcdb299-3f74-49d2-8b56-cae72a7dfa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b71c64-b4ec-4070-aada-cd382529cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "    precision = round(precision_score(y_test, y_pred), 3)\n",
    "    recall = round(recall_score(y_test, y_pred), 3)\n",
    "    f1 = round(f1_score(y_test, y_pred), 3)\n",
    "\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc = round(roc_auc_score(y_test, y_prob), 3)\n",
    "\n",
    "    evaluation_metrics = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"AUC\": auc,\n",
    "        \"Time\": round(execution_time, 3),\n",
    "    }\n",
    "\n",
    "    return evaluation_metrics\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_all_models(models_dict, X_train, y_train, X_test, y_test):\n",
    "    evaluation_results = {}\n",
    "    for model_name, model in models_dict.items():\n",
    "        evaluation_metrics = train_and_evaluate_model(\n",
    "            model, X_train, y_train, X_test, y_test\n",
    "        )\n",
    "        evaluation_results[model_name] = evaluation_metrics\n",
    "\n",
    "    results_df = pd.DataFrame.from_dict(evaluation_results, orient=\"index\")\n",
    "    return results_df\n",
    "\n",
    "def preprocess_applier(preprocessor, X_data):\n",
    "    # Apply preprocessing to the data\n",
    "    X_data_processed = preprocessor.transform(X_data)\n",
    "\n",
    "    # Get column names after preprocessing\n",
    "    numeric_feature_names = preprocessor.transformers_[0][-1]\n",
    "    categorical_feature_names = preprocessor.transformers_[1][-1]\n",
    "\n",
    "    # Get the unique categories of the categorical variables\n",
    "    unique_categories = preprocessor.named_transformers_[\"cat\"][\"onehot\"].categories_\n",
    "\n",
    "    # Create column names after OneHotEncoding\n",
    "    encoded_categorical_feature_names = []\n",
    "    for i, categories in enumerate(unique_categories):\n",
    "        for category in categories:\n",
    "            encoded_categorical_feature_names.append(\n",
    "                f\"{categorical_feature_names[i]}_{category}\"\n",
    "            )\n",
    "\n",
    "    # Convert the sparse matrix to a Pandas DataFrame\n",
    "    transformed_df = pd.DataFrame(\n",
    "        X_data_processed.toarray(),\n",
    "        columns=numeric_feature_names + encoded_categorical_feature_names,\n",
    "    )\n",
    "\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c8436-c6e0-4e6b-b9c5-7ed5c962d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Read Data\")\n",
    "\n",
    "# Paths\n",
    "path_raw = \"../../data/raw/\"\n",
    "path_processed = \"../../data/processed/\"\n",
    "path_final = \"../../data/final/\"\n",
    "\n",
    "# Read data\n",
    "train = pd.read_csv(path_processed + \"train.csv\")\n",
    "test = pd.read_csv(path_processed + \"test.csv\")\n",
    "\n",
    "columns_to_convert = ['Pclass', 'SibSp', 'Parch']\n",
    "train[columns_to_convert] = train[columns_to_convert].astype(str)\n",
    "test[columns_to_convert] = test[columns_to_convert].astype(str)\n",
    "\n",
    "# Get column names by data types\n",
    "target_variable = 'Survived'\n",
    "\n",
    "float_columns = [x for x in list(train.select_dtypes(include=['float64']).columns) if x != target_variable]\n",
    "integer_columns = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x != target_variable]\n",
    "object_columns = [x for x in list(train.select_dtypes(include=['object']).columns) if x != target_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7992ce-d307-41b4-be00-c857ccc4e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Split the dataset into training and testing sets\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "target = 'Survived'\n",
    "index_column = 'PassengerId'\n",
    "\n",
    "features = [x for x in train.columns if x not in [target, index_column]]\n",
    "\n",
    "X = train[features]\n",
    "y = train[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Example of numeric and categorical variables\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Create transformers for numeric and categorical variables\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Create the ColumnTransformer to apply transformations in a pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply preprocessing to the training and testing data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get column names after preprocessing\n",
    "numeric_feature_names = preprocessor.transformers_[0][-1]\n",
    "categorical_feature_names = preprocessor.transformers_[1][-1]\n",
    "\n",
    "# Get the unique categories of the categorical variables\n",
    "unique_categories = preprocessor.named_transformers_['cat']['onehot'].categories_\n",
    "\n",
    "# Create column names after OneHotEncoding\n",
    "encoded_categorical_feature_names = []\n",
    "for i, categories in enumerate(unique_categories):\n",
    "    for category in categories:\n",
    "        encoded_categorical_feature_names.append(f'{categorical_feature_names[i]}_{category}')\n",
    "\n",
    "# Convert the sparse matrix to a Pandas DataFrame\n",
    "transformed_train_df = pd.DataFrame(X_train_processed.toarray(), columns=numeric_feature_names + encoded_categorical_feature_names)\n",
    "transformed_test_df = pd.DataFrame(X_test_processed.toarray(), columns=numeric_feature_names + encoded_categorical_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8fbb3-f738-403c-bbc2-82ba985ef74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models with optimized hyperparameters\n",
    "\n",
    "# Initialize RandomForest classifier with optimized hyperparameters\n",
    "random_forest = RandomForestClassifier(random_state=42,\n",
    "                                       n_estimators=100,\n",
    "                                       max_depth=None,\n",
    "                                       min_samples_split=2,\n",
    "                                       min_samples_leaf=1)\n",
    "\n",
    "# Initialize LGBM classifier with optimized hyperparameters\n",
    "lgbm = LGBMClassifier(random_state=42,\n",
    "                      n_estimators=100,\n",
    "                      learning_rate=0.1,\n",
    "                      max_depth=-1)\n",
    "\n",
    "# Initialize DecisionTree classifier with optimized hyperparameters\n",
    "decision_tree = DecisionTreeClassifier(random_state=42,\n",
    "                                       max_depth=None,\n",
    "                                       min_samples_split=2,\n",
    "                                       min_samples_leaf=1)\n",
    "\n",
    "# Initialize KNeighbors classifier with optimized hyperparameters\n",
    "knn = KNeighborsClassifier(n_neighbors=5,\n",
    "                           weights='uniform',\n",
    "                           p=2)\n",
    "\n",
    "# Initialize LogisticRegression classifier with optimized hyperparameters\n",
    "logistic_regression = LogisticRegression(random_state=42,\n",
    "                                         C=1.0,\n",
    "                                         penalty='l2')\n",
    "\n",
    "# Initialize GaussianNB classifier with optimized hyperparameters\n",
    "gaussian_nb = GaussianNB(var_smoothing=1e-9)\n",
    "\n",
    "# Initialize AdaBoost classifier with optimized hyperparameters\n",
    "ada_boost = AdaBoostClassifier(random_state=42,\n",
    "                               n_estimators=50,\n",
    "                               learning_rate=0.1)\n",
    "\n",
    "# Create a dictionary of models with their parameters for easy iteration\n",
    "models = {\n",
    "    'Random Forest': random_forest, \n",
    "    'LGBM': lgbm,\n",
    "    'Decision Tree': decision_tree,\n",
    "    'KNN': knn, \n",
    "    'Logistic Regression': logistic_regression,\n",
    "    'GaussianNB': gaussian_nb,\n",
    "    'AdaBoost': ada_boost\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d513b9-e9a1-4956-a47d-d276b0781ab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the function to train and evaluate all models\n",
    "logger.info(\"Train and evaluate all models\")\n",
    "results_df = train_and_evaluate_all_models(models, transformed_train_df, y_train, transformed_test_df, y_test).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618fa88d-0b28-4f81-85ae-921a6cc0f630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the DataFrame with the results\n",
    "logger.info(\"Sort the results by the AUC metric\")\n",
    "results_df = results_df.sort_values('AUC', ascending=False).rename(columns = {'index': 'Models'})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69597c-b56d-4c96-ba34-812c72ad3b97",
   "metadata": {},
   "source": [
    "**Model Performance Evaluation Results**\n",
    "\n",
    "The table below presents the performance metrics for various machine learning models applied to the Titanic dataset. The metrics include Accuracy, Precision, Recall, F1-Score, AUC (Area Under the ROC Curve), and the Time taken for training and evaluation. Each metric provides insights into different aspects of model performance.\n",
    "\n",
    "\n",
    "**Explanation of Metrics:**\n",
    "\n",
    "- **Accuracy:** The proportion of correctly classified instances among the total instances. A higher value indicates better overall performance.\n",
    "- **Precision:** The proportion of true positive predictions among all positive predictions. It reflects the model's ability to avoid false positives.\n",
    "- **Recall:** The proportion of true positive predictions among all actual positives. It indicates the model's ability to capture all relevant instances (sensitivity).\n",
    "- **F1-Score:** The harmonic mean of precision and recall, providing a balance between the two. It is particularly useful when the class distribution is imbalanced.\n",
    "- **AUC (Area Under the ROC Curve):** Measures the model's ability to distinguish between classes. A higher AUC value indicates better performance.\n",
    "- **Time:** The time taken to train and evaluate the model.\n",
    "\n",
    "**Model Insights:**\n",
    "\n",
    "1. **Random Forest:** Achieved a high AUC of 0.886, indicating excellent discrimination between classes, with a good balance of precision and recall.\n",
    "2. **Logistic Regression:** Performed similarly to Random Forest with high accuracy and a strong F1-Score, but slightly lower AUC.\n",
    "3. **KNN:** Showed the highest accuracy and a strong F1-Score, but with a marginally lower AUC compared to Random Forest and Logistic Regression.\n",
    "4. **LGBM:** Performed well but with a slightly lower accuracy and AUC than Random Forest, Logistic Regression, and KNN.\n",
    "5. **AdaBoost:** Had decent performance but was slightly less effective in terms of precision and AUC compared to the top models.\n",
    "6. **Decision Tree:** Demonstrated good precision and recall but with a significantly lower AUC.\n",
    "7. **GaussianNB:** Had the lowest performance metrics, indicating poor model performance, especially with a very high recall but low precision and AUC.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Among the models evaluated, the Random Forest, Logistic Regression, and KNN classifiers showed the best overall performance, with high accuracy, precision, recall, F1-Score, and AUC values. Random Forest had the highest AUC, making it the best model for distinguishing between classes. Logistic Regression and KNN also performed well, with KNN achieving the highest accuracy. The time metric indicates that Logistic Regression is the fastest to train and evaluate, followed by KNN, making them efficient choices for quick model training. GaussianNB showed the poorest performance, highlighting its unsuitability for this specific classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b60953-d65a-44e3-a564-81c6b17fb85f",
   "metadata": {},
   "source": [
    "## Utilizing the Best Model\n",
    "\n",
    "In this section, we will focus on leveraging the best-performing machine learning model identified during our evaluation phase. Based on our performance metrics, the Random Forest classifier demonstrated superior results with the highest AUC and a strong balance of precision and recall. We will now utilize this model to make predictions on new data and explore its potential applications. This includes deploying the model, interpreting its predictions, and assessing its real-world impact. By harnessing the power of the best model, we aim to achieve accurate and actionable insights from the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76377de3-8591-41a4-a192-b697d69a9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Select model\")\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model.fit(transformed_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb7fbc-2c94-4200-a3d0-1e6024cb1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hyperparameters\n",
    "hyperparameters = model.get_params()\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e42b7-f290-431d-bef5-ce061041f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = transformed_train_df.columns\n",
    "\n",
    "# Create a DataFrame with features and their importance\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the most important features\n",
    "feature_importance_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137997f7-057a-4f97-a8ca-8fdd6f4b9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot to visualize the most important features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'][:10], feature_importance_df['Importance'][:10], color='#bce4b5')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis so the most important features are at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d588a942-8ad2-4580-99cf-dd28ecc4f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Make predictions\")\n",
    "\n",
    "X_test_processed2 = preprocess_applier(preprocessor, test.drop('PassengerId', axis=1))\n",
    "predictions = model.predict(X_test_processed2)\n",
    "test[\"Survived\"] = predictions\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2b82c-2ca9-4bae-a798-fb1b25726dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Save Results\")\n",
    "\n",
    "logger.info(\"Save prediction results\")\n",
    "test.to_csv(path_final + \"predictions.csv\", index=False, sep=',')\n",
    "\n",
    "logger.info(\"Save results of the different models\")\n",
    "results_df.to_csv(path_final + \"models_metrics.csv\", index=False, sep=',')\n",
    "\n",
    "logger.info(\"Save results of the best models\")\n",
    "feature_importance_df.to_csv(path_final + \"feature_importance_df.csv\", index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a97b245-909d-4217-902d-bb91e52915c9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "Throughout the machine learning phase of our Titanic dataset analysis, we successfully built and evaluated several predictive models to determine passenger survival. The process involved several critical steps:\n",
    "\n",
    "1. **Data Preprocessing:** We prepared the data by handling missing values, encoding categorical variables, and scaling numerical features. This ensured that our data was clean and suitable for model training.\n",
    "\n",
    "2. **Model Selection and Training:** We experimented with various machine learning algorithms, including Random Forest, Logistic Regression, K-Nearest Neighbors (KNN), LightGBM, AdaBoost, Decision Tree, and Gaussian Naive Bayes. Each model was trained using optimized hyperparameters to enhance performance.\n",
    "\n",
    "3. **Evaluation Metrics:** We evaluated the models based on key metrics such as Accuracy, Precision, Recall, F1-Score, and AUC (Area Under the ROC Curve). This comprehensive evaluation allowed us to identify the strengths and weaknesses of each model.\n",
    "\n",
    "4. **Best Model Identification:** Among the models, the Random Forest classifier emerged as the best-performing model with the highest AUC and a strong balance of precision and recall. This model demonstrated superior ability to distinguish between passengers who survived and those who did not.\n",
    "\n",
    "5. **Feature Importance:** Using the Random Forest model, we identified the most important features contributing to the prediction of survival. This insight helps in understanding the factors that significantly influenced the survival chances of passengers.\n",
    "\n",
    "6. **Predictions:** We utilized the best-performing model to make predictions on the test dataset, providing an actionable outcome based on our analysis.\n",
    "\n",
    "7. **Results Documentation:** Finally, we saved the prediction results and the evaluation metrics of all models. This documentation ensures reproducibility and allows for further analysis and refinement.\n",
    "\n",
    "Overall, the machine learning phase has provided us with valuable predictive insights and a robust model for assessing passenger survival on the Titanic. The comprehensive approach, from data preprocessing to model evaluation and deployment, underscores the importance of methodical and thorough analysis in achieving accurate and meaningful results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
