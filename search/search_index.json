{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Tintanic Challenge!","text":""},{"location":"#sections","title":"Sections","text":"<p>EDA</p><p>Titanic EDA</p> <p>FE</p><p>Titanic Feature Engineering</p> <p>MLE</p><p>Titanic Machine Learning</p>"},{"location":"data/","title":"Dataset Description","text":""},{"location":"data/#overview","title":"Overview","text":"<p>The data has been split into two groups:</p> <ul> <li>training set (train.csv)</li> <li>test set (test.csv)</li> </ul> <p>The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.</p> <p>The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.</p> <p>We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.</p>"},{"location":"data/#data-dictionary","title":"Data Dictionary","text":"Variable Definition Key survival Survival 0 = No, 1 = Yes pclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd sex Sex Age Age in years sibsp # of siblings / spouses aboard the Titanic parch # of parents / children aboard the Titanic ticket Ticket number fare Passenger fare cabin Cabin number embarked Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton"},{"location":"data/#variable-notes","title":"Variable Notes","text":"<p>pclass: A proxy for socio-economic status (SES) </p> <ul> <li>1st = Upper  </li> <li>2nd = Middle  </li> <li>3rd = Lower  </li> </ul> <p>age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5  </p> <p>sibsp: The dataset defines family relations in this way...  </p> <ul> <li>Sibling = brother, sister, stepbrother, stepsister  </li> <li>Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)  </li> </ul> <p>parch: The dataset defines family relations in this way...  </p> <ul> <li>Parent = mother, father  </li> <li>Child = daughter, son, stepdaughter, stepson  </li> <li>Some children travelled only with a nanny, therefore parch=0 for them.</li> </ul>"},{"location":"overview/","title":"Titanic - Machine Learning from Disaster","text":""},{"location":"overview/#description","title":"Description","text":""},{"location":"overview/#ahoy-welcome-to-kaggle-youre-in-the-right-place","title":"\ud83d\udc4b\ud83d\udef3\ufe0f Ahoy, welcome to Kaggle! You\u2019re in the right place.","text":"<p>This is the legendary Titanic ML competition \u2013 the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.</p> <p>If you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle</p> <p>The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.</p> <p>Read on or watch the video below to explore more details. Once you\u2019re ready to start competing, click on the \"Join Competition button to create an account and gain access to the competition data. Then check out Alexis Cook\u2019s Titanic Tutorial that walks you through step by step how to make your first submission!</p> <p></p>"},{"location":"overview/#the-challenge","title":"The Challenge","text":"<p>The sinking of the Titanic is one of the most infamous shipwrecks in history.</p> <p>On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.</p> <p>While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.</p> <p>In this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).</p> <p>Recommended Tutorial  We highly recommend Alexis Cook\u2019s Titanic Tutorial that walks you through making your very first submission step by step and this starter notebook to get started.</p>"},{"location":"overview/#overview-of-how-kaggles-competitions-work","title":"Overview of How Kaggle\u2019s Competitions Work","text":"<ol> <li>Join the Competition     Read about the challenge description, accept the Competition Rules and gain access to the competition dataset.</li> <li>Get to Work     Download the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.</li> <li>Make a Submission     Upload your prediction as a submission on Kaggle and receive an accuracy score.</li> <li>Check the Leaderboard     See how your model ranks against other Kagglers on our leaderboard.</li> <li>Improve Your Score     Check out the discussion forum to find lots of tutorials and insights from other competitors.</li> </ol> <p>Kaggle Lingo Video  You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out Dr. Rachael Tatman\u2019s video on Kaggle Lingo to get up to speed!</p>"},{"location":"overview/#what-data-will-i-use-in-this-competition","title":"What Data Will I Use in This Competition?","text":"<p>In this competition, you\u2019ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled <code>train.csv</code> and the other is titled <code>test.csv</code>.</p> <p><code>Train.csv</code> will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d.</p> <p>The <code>test.csv</code> dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger. It\u2019s your job to predict these outcomes.</p> <p>Using the patterns you find in the <code>train.csv</code> data, predict whether the other 418 passengers on board (found in <code>test.csv</code>) survived.</p> <p>Check out the \u201cData\u201d tab to explore the datasets even further. Once you feel you\u2019ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.</p>"},{"location":"overview/#how-to-submit-your-prediction-to-kaggle","title":"How to Submit your Prediction to Kaggle","text":"<p>Once you\u2019re ready to make a submission and get on the leaderboard:</p> <ol> <li>Click on the \u201cSubmit Predictions\u201d button </li> <li>Upload a CSV file in the submission file format. You\u2019re able to submit 10 submissions a day. </li> </ol>"},{"location":"overview/#submission-file-format","title":"Submission File Format:","text":"<p>You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond <code>PassengerId</code> and <code>Survived</code>) or rows.</p> <p>The file should have exactly 2 columns:</p> <ul> <li><code>PassengerId</code> (sorted in any order)</li> <li><code>Survived</code> (contains your binary predictions: 1 for survived, 0 for deceased)</li> </ul>"},{"location":"overview/#got-it-im-ready-to-get-started-where-do-i-get-help-if-i-need-it","title":"Got it! I\u2019m ready to get started. Where do I get help if I need it?","text":"<ul> <li>For Competition Help: Titanic Discussion Forum</li> <li>Technical Help: Kaggle Contact Us Page</li> </ul> <p>Kaggle doesn\u2019t have a dedicated support team so you\u2019ll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p>"},{"location":"overview/#a-last-word-on-kaggle-notebooks","title":"A Last Word on Kaggle Notebooks","text":"<p>As we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data &amp; code.</p> <p>In every competition, you\u2019ll find many Notebooks shared with incredible insights. It\u2019s an invaluable resource worth becoming familiar with. Check out this competition\u2019s Notebooks here.</p>"},{"location":"overview/#ready-to-compete-join-the-competition-here","title":"\ud83c\udfc3\u200d\u2640Ready to Compete? Join the Competition Here!","text":""},{"location":"overview/#evaluation","title":"Evaluation","text":""},{"location":"overview/#goal","title":"Goal","text":"<p>It is your job to predict if a passenger survived the sinking of the Titanic or not. For each in the test set, you must predict a 0 or 1 value for the variable.</p>"},{"location":"overview/#metric","title":"Metric","text":"<p>Your score is the percentage of passengers you correctly predict. This is known as accuracy.</p>"},{"location":"overview/#submission-file-format_1","title":"Submission File Format","text":"<p>You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.  </p> <p>The file should have exactly 2 columns:</p> <ul> <li>PassengerId (sorted in any order)</li> <li>Survived (contains your binary predictions: 1 for survived, 0 for deceased)</li> </ul> <p>PassengerId,Survived  892,0   893,1   894,0   Etc.</p> <p>You can download an example submission file (gender_submission.csv) on the Data page.</p>"},{"location":"overview/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"overview/#what-is-a-getting-started-competition","title":"What is a Getting Started competition?","text":"<p>Getting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.</p> <p>Getting Started competitions are a non-competitive way to get familiar with Kaggle\u2019s platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.</p>"},{"location":"overview/#how-do-i-create-and-manage-a-team","title":"How do I create and manage a team?","text":"<p>When you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More &gt; Team page.</p> <p>We've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the discussion forum.</p>"},{"location":"overview/#what-are-notebooks","title":"What are Notebooks?","text":"<p>Kaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the Notebooks tab to view all of the publicly shared code for the Titanic competition. For more on how to use Notebooks to learn data science, check out our Courses!</p>"},{"location":"overview/#why-did-my-team-disappear-from-the-leaderboard","title":"Why did my team disappear from the leaderboard?","text":"<p>To keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.</p> <p>If your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.</p> <p>\"I worked so hard to get that score! Give it back!\" Read more about our decision to implement a rolling leaderboard here.</p>"},{"location":"overview/#how-do-i-contact-support","title":"How do I contact Support?","text":"<p>Kaggle does not have a dedicated support team so you\u2019ll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you\u2019ll want to use the Titanic discussion forum).</p> <p>Support is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can\u2019t find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!</p> <p>If your problem persists or it seems to be effective all participants then please contact us.</p>"},{"location":"overview/#citation","title":"Citation","text":"<ul> <li>Will Cukierski. (2012). Titanic - Machine Learning from Disaster. Kaggle. https://kaggle.com/competitions/titanic</li> </ul>"},{"location":"project/01_eda/","title":"\ud83d\udcca Exploratory Data Analysis","text":"In\u00a0[1]: Copied! <pre># librerias\nfrom loguru import logger\nimport pandas as pd\nfrom IPython.display import display\nfrom utils import * \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n</pre> # librerias from loguru import logger import pandas as pd from IPython.display import display from utils import *   import warnings warnings.filterwarnings(\"ignore\") pd.set_option('display.max_columns', 500) pd.set_option('display.max_rows', 500) pd.set_option('display.float_format', lambda x: '%.3f' % x) In\u00a0[2]: Copied! <pre>logger.info(\"Read Data\")\n\n# paths\npath_raw = \"../../data/raw/\"\npath_procesed = \"../../data/processed/\"\npath_final = \"../../data/final/\"\n\n# read data\ntrain = pd.read_csv(path_raw + \"train.csv\")\ntest = pd.read_csv(path_raw + \"test.csv\")\n</pre> logger.info(\"Read Data\")  # paths path_raw = \"../../data/raw/\" path_procesed = \"../../data/processed/\" path_final = \"../../data/final/\"  # read data train = pd.read_csv(path_raw + \"train.csv\") test = pd.read_csv(path_raw + \"test.csv\") <pre>2024-06-09 17:53:26.906 | INFO     | __main__:&lt;module&gt;:1 - Read Data\n</pre> In\u00a0[3]: Copied! <pre># information about the data types and non-null values in each column\nprint(\"TRAIN:\")\ntrain.info()\n</pre> # information about the data types and non-null values in each column print(\"TRAIN:\") train.info() <pre>TRAIN:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n</pre> <p>\ud83d\udd11Note: For now, we will focus on the train dataset, but the same should be done for the test dataset.</p> In\u00a0[4]: Copied! <pre># check for duplicates in the dataset\nduplicates = train.duplicated()\n\n# count the number of duplicate rows\nnum_duplicates = duplicates.sum()\nprint(\"duplicate rows:\", num_duplicates)\n</pre> # check for duplicates in the dataset duplicates = train.duplicated()  # count the number of duplicate rows num_duplicates = duplicates.sum() print(\"duplicate rows:\", num_duplicates) <pre>duplicate rows: 0\n</pre> <p>To perform an Exploratory Data Analysis (EDA) with visualizations, whether univariate, bivariate or not, it is essential to consider the type of data we are working with. Additionally, we can perform a deep scan of all columns or an in-depth scan of individual columns, depending on the desired speed and detail of our EDA. The goal is to present all findings in a detailed and clear manner to ensure maximum understanding.</p> In\u00a0[5]: Copied! <pre># get column names by data types\ntarget = 'Survived'\n\nfloat_columns = [x for x in list(train.select_dtypes(include=['float64']).columns) if x != target]\ninteger_columns = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x != target]\nobject_columns = [x for x in list(train.select_dtypes(include=['object']).columns) if x != target]\n\n# display column names by data type\nprint(f\"Target: {target}\")\nprint()\nprint(\"Total float columns:\", len(float_columns))\nprint(\"Float columns:\", float_columns)\nprint()\nprint(\"Total integer columns:\", len(integer_columns))\nprint(\"Integer columns:\", integer_columns)\nprint()\nprint(\"Total object columns:\", len(object_columns))\nprint(\"Object columns:\", object_columns)\n</pre> # get column names by data types target = 'Survived'  float_columns = [x for x in list(train.select_dtypes(include=['float64']).columns) if x != target] integer_columns = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x != target] object_columns = [x for x in list(train.select_dtypes(include=['object']).columns) if x != target]  # display column names by data type print(f\"Target: {target}\") print() print(\"Total float columns:\", len(float_columns)) print(\"Float columns:\", float_columns) print() print(\"Total integer columns:\", len(integer_columns)) print(\"Integer columns:\", integer_columns) print() print(\"Total object columns:\", len(object_columns)) print(\"Object columns:\", object_columns) <pre>Target: Survived\n\nTotal float columns: 2\nFloat columns: ['Age', 'Fare']\n\nTotal integer columns: 4\nInteger columns: ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n\nTotal object columns: 5\nObject columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n</pre> <p>En este caso, la columna 'Age' deber\u00eda ser entera, pero tiene datos Nulos (o <code>NaN</code>), por lo cual se convierte autom\u00e1ticamente en una columna tipo <code>float</code>. De momento tratemosla como una columna tipo <code>float</code>.</p> <p>There are columns used to identify each individual, typically indicated by the term 'ID' in their name. It is essential that these identifiers do not have a value of zero.</p> <p>It is important to note that these identifiers should not be duplicated unless there is more than one record due to analysis in relation to other columns (e.g., the period). In these specific cases, duplication may be relevant and is associated with certain analytical contexts involving other variables or time periods.</p> In\u00a0[6]: Copied! <pre>logger.info(\"EDA\")\nlogger.info('PassengerId')\ntotal_nulls = train['PassengerId'].isnull().sum()\nprint(f\"Total null values: {total_nulls} \")\n</pre> logger.info(\"EDA\") logger.info('PassengerId') total_nulls = train['PassengerId'].isnull().sum() print(f\"Total null values: {total_nulls} \") <pre>2024-06-09 17:53:26.967 | INFO     | __main__:&lt;module&gt;:1 - EDA\n2024-06-09 17:53:26.968 | INFO     | __main__:&lt;module&gt;:2 - PassengerId\n</pre> <pre>Total null values: 0 \n</pre> In\u00a0[7]: Copied! <pre># Set as index\ntrain = train.set_index('PassengerId')\ntrain.head()\n</pre> # Set as index train = train.set_index('PassengerId') train.head() Out[7]: Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked PassengerId 1 0 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C 3 1 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S 5 0 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S <p>Now, we will proceed to work on the remaining columns.</p> <p>To analyze variables with float values, the approach depends on the distribution of the data. For univariate analysis, a Histogram is usually used as a starting point (First Case). However, if the dataset is extensive or there is a noticeable concentration of values around zero (a common scenario), it is more effective to transform the data into discrete intervals (Second Case).</p> <p>The definition of these intervals can be done automatically using a function that generates equidistant ranges. However, sometimes it is preferable to define these intervals manually, as automation could create numerous bins, making data interpretation and analysis more difficult. In such cases, a manual approach allows for adjusting the intervals more appropriately according to the specific nature of the data, facilitating better understanding and analysis.</p> In\u00a0[8]: Copied! <pre>logger.info(f\"floats: {float_columns}\")\n\nbins = {\n    'Age': [0, 10, 20, 30, 40, 50, 60, 70, 80, 90],\n    'Fare':  [0, 10, 25, 50, 100, 1000]\n}\n\ntarget = 'Survived'\n\nfor col in float_columns: \n    print(f\"Column: {col}\\n\")\n    print(\"Univariate Analysis\")\n    plot_histogram(train, col)\n    plot_range_distribution(train, col, bins[col], figsize=(10, 5))\n    print(\"Bivariate Analysis\")\n    plot_histogram_vo(train, col, target)\n    plot_range_distribution_vo(train, col, bins[col], target, figsize=(10, 5))\n    print(\"Tables\")\n    table = calculate_percentage_vo(train, col, bins[col], target)\n    display(table)\n</pre> logger.info(f\"floats: {float_columns}\")  bins = {     'Age': [0, 10, 20, 30, 40, 50, 60, 70, 80, 90],     'Fare':  [0, 10, 25, 50, 100, 1000] }  target = 'Survived'  for col in float_columns:      print(f\"Column: {col}\\n\")     print(\"Univariate Analysis\")     plot_histogram(train, col)     plot_range_distribution(train, col, bins[col], figsize=(10, 5))     print(\"Bivariate Analysis\")     plot_histogram_vo(train, col, target)     plot_range_distribution_vo(train, col, bins[col], target, figsize=(10, 5))     print(\"Tables\")     table = calculate_percentage_vo(train, col, bins[col], target)     display(table) <pre>2024-06-09 17:53:27.000 | INFO     | __main__:&lt;module&gt;:1 - floats: ['Age', 'Fare']\n</pre> <pre>Column: Age\n\nUnivariate Analysis\n</pre> <pre>Bivariate Analysis\n</pre> <pre>Tables\n</pre> Count Percentage Survived 0 1 0 1 AgeRange [0, 10) 24.000 38.000 0.057 0.131 [10, 20) 61.000 41.000 0.144 0.141 [20, 30) 143.000 77.000 0.337 0.266 [30, 40) 94.000 73.000 0.222 0.252 [40, 50) 55.000 34.000 0.130 0.117 [50, 60) 28.000 20.000 0.066 0.069 [60, 70) 13.000 6.000 0.031 0.021 [70, 80) 6.000 0.000 0.014 0.000 [80, 90) 0.000 1.000 0.000 0.003 <pre>Column: Fare\n\nUnivariate Analysis\n</pre> <pre>Bivariate Analysis\n</pre> <pre>Tables\n</pre> Count Percentage Survived 0 1 0 1 FareRange [0, 10) 269.000 67.000 0.490 0.196 [10, 25) 128.000 93.000 0.233 0.272 [25, 50) 100.000 73.000 0.182 0.213 [50, 100) 38.000 70.000 0.069 0.205 [100, 1000) 14.000 39.000 0.026 0.114 <p>To visually represent variables of type <code>int</code> or <code>object</code>, it is initially recommended to use the <code>value_counts</code> method from Pandas to count the unique values in that column. However, different considerations should be taken into account:</p> <ul> <li>When the number of unique values is small, it is appropriate to use <code>value_counts</code> directly for both <code>int</code> and <code>object</code> type variables.</li> <li>For <code>int</code> type variables with a large number of categories, it is useful to work with value intervals before creating graphical visualizations.</li> <li>For <code>object</code> type variables with multiple categories, it may be helpful to prioritize the most frequent ones and group the rest under a general label such as \"others\". However, if most values are unique, that variable may not provide relevant information for graphical representation (e.g., information such as addresses, phone numbers, emails, etc.).</li> </ul> In\u00a0[9]: Copied! <pre>logger.info(f\"Integers: {integer_columns}\")\nlogger.info(f\"Objects: {object_columns}\")\n\nfor col in ['Pclass', 'SibSp', 'Parch', 'Sex', 'Embarked']:\n    print(col)\n    print(\"Univariate Analysis\")\n    plot_barplot(train, col)\n    print(\"Bivariate Analysis\")\n    plot_barplot_vo(train, col, target, figsize=(10, 5))\n    print(\"Tables\")\n    table = calculate_percentage_vo_int(train, col, target).fillna(0)\n    display(table)\n</pre> logger.info(f\"Integers: {integer_columns}\") logger.info(f\"Objects: {object_columns}\")  for col in ['Pclass', 'SibSp', 'Parch', 'Sex', 'Embarked']:     print(col)     print(\"Univariate Analysis\")     plot_barplot(train, col)     print(\"Bivariate Analysis\")     plot_barplot_vo(train, col, target, figsize=(10, 5))     print(\"Tables\")     table = calculate_percentage_vo_int(train, col, target).fillna(0)     display(table) <pre>2024-06-09 17:53:28.153 | INFO     | __main__:&lt;module&gt;:1 - Integers: ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n2024-06-09 17:53:28.154 | INFO     | __main__:&lt;module&gt;:2 - Objects: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n</pre> <pre>Pclass\nUnivariate Analysis\n</pre> <pre>Bivariate Analysis\n</pre> <pre>Tables\n</pre> Count Percentage Survived 0 1 0 1 Pclass 1 80.000 136.000 0.146 0.398 2 97.000 87.000 0.177 0.254 3 372.000 119.000 0.678 0.348 <pre>SibSp\nUnivariate Analysis\n</pre> <pre>Bivariate Analysis\n</pre> <pre>Tables\n</pre> Count Percentage Survived 0 1 0 1 SibSp 0 398.000 210.000 0.725 0.614 1 97.000 112.000 0.177 0.327 2 15.000 13.000 0.027 0.038 3 12.000 4.000 0.022 0.012 4 15.000 3.000 0.027 0.009 5 5.000 0.000 0.009 0.000 8 7.000 0.000 0.013 0.000 <pre>Parch\nUnivariate Analysis\n</pre> <pre>Bivariate Analysis\n</pre> <pre>Tables\n</pre> Count Percentage Survived 0 1 0 1 Parch 0 445.000 233.000 0.811 0.681 1 53.000 65.000 0.097 0.190 2 40.000 40.000 0.073 0.117 3 2.000 3.000 0.004 0.009 4 4.000 0.000 0.007 0.000 5 4.000 1.000 0.007 0.003 6 1.000 0.000 0.002 0.000 <pre>Sex\nUnivariate Analysis\n</pre> <pre>Bivariate Analysis\n</pre> <pre>Tables\n</pre> Count Percentage Survived 0 1 0 1 Sex female 81.000 233.000 0.148 0.681 male 468.000 109.000 0.852 0.319 <pre>Embarked\nUnivariate Analysis\n</pre> <pre>Bivariate Analysis\n</pre> <pre>Tables\n</pre> Count Percentage Survived 0 1 0 1 Embarked C 75.000 93.000 0.137 0.274 Q 47.000 30.000 0.086 0.088 S 427.000 217.000 0.778 0.638 <p>To perform exploratory analysis on the 'Cabin', 'Name', and 'Ticket' columns in the Titanic dataset, you can follow various approaches depending on the information they contain and the specific objectives of your analysis.</p>"},{"location":"project/01_eda/#exploratory-data-analysis","title":"\ud83d\udcca Exploratory Data Analysis\u00b6","text":""},{"location":"project/01_eda/#introduction","title":"Introduction\u00b6","text":"<p>The Titanic dataset is one of the most well-known datasets in the field of data science and machine learning. It contains detailed information about the passengers aboard the Titanic, a British passenger liner that tragically sank in the North Atlantic Ocean on April 15, 1912, after hitting an iceberg. This disaster resulted in the loss of over 1,500 lives and has since become a poignant example of maritime tragedy.</p> <p>Exploratory Data Analysis (EDA) is a crucial step in any data analysis project. It involves examining the dataset to uncover underlying patterns, spot anomalies, test hypotheses, and check assumptions through summary statistics and graphical representations. For the Titanic dataset, EDA helps us understand the factors that influenced survival rates, such as passenger demographics, socio-economic status, and travel details.</p> <p>The dataset comprises variables such as passenger age, gender, ticket class, fare paid, and whether or not the passenger survived. By analyzing these variables, we can gain insights into which groups of passengers were more likely to survive and the reasons behind these trends. For instance, we might explore questions like:</p> <ul> <li>Did gender play a significant role in survival rates?</li> <li>Were first-class passengers more likely to survive than those in lower classes?</li> <li>How did the age of passengers affect their chances of survival?</li> </ul> <p>Through various visualizations and statistical analyses, EDA provides a foundation for more complex modeling and predictive analysis. It allows us to clean and preprocess the data, handle missing values, and create new features that might improve the performance of machine learning models.</p>"},{"location":"project/01_eda/#about-data","title":"About Data\u00b6","text":""},{"location":"project/01_eda/#about-eda","title":"About EDA\u00b6","text":""},{"location":"project/01_eda/#conclusion","title":"Conclusion\u00b6","text":"<p>In conclusion, we conducted an Exploratory Data Analysis (EDA) on the Titanic dataset, focusing on understanding the characteristics and distribution of various columns. We identified the data types and non-null values, checked for duplicates, and performed univariate and bivariate analyses on numerical and categorical variables. Through graphical representations and interval-based transformations, we gained insights into the data's structure and key factors influencing survival rates. This comprehensive EDA serves as a foundation for further in-depth analysis and modeling.</p>"},{"location":"project/02_fe/","title":"\ud83d\udcdd Feature Engineering","text":"In\u00a0[1]: Copied! <pre># Libraries\nfrom loguru import logger\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n</pre> # Libraries from loguru import logger import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  import warnings warnings.filterwarnings(\"ignore\") pd.set_option('display.max_columns', 500) pd.set_option('display.max_rows', 500) pd.set_option('display.float_format', lambda x: '%.3f' % x) In\u00a0[2]: Copied! <pre>logger.info(\"Read Data\")\n\n# Paths\npath_raw = \"../../data/raw/\"\npath_processed = \"../../data/processed/\"\npath_final = \"../../data/final/\"\n\n# Read data\ntrain = pd.read_csv(path_raw + \"train.csv\")\ntest = pd.read_csv(path_raw + \"test.csv\")\n</pre> logger.info(\"Read Data\")  # Paths path_raw = \"../../data/raw/\" path_processed = \"../../data/processed/\" path_final = \"../../data/final/\"  # Read data train = pd.read_csv(path_raw + \"train.csv\") test = pd.read_csv(path_raw + \"test.csv\") <pre>2024-06-09 18:03:20.901 | INFO     | __main__:&lt;module&gt;:1 - Read Data\n</pre> In\u00a0[3]: Copied! <pre># Get column names by data types\ntarget = 'Survived'\n\nfloat_columns = [x for x in list(train.select_dtypes(include=['float64']).columns) if x != target]\ninteger_columns = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x != target]\nobject_columns = [x for x in list(train.select_dtypes(include=['object']).columns) if x != target]\n</pre> # Get column names by data types target = 'Survived'  float_columns = [x for x in list(train.select_dtypes(include=['float64']).columns) if x != target] integer_columns = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x != target] object_columns = [x for x in list(train.select_dtypes(include=['object']).columns) if x != target] In\u00a0[4]: Copied! <pre>logger.info(\"Remove variables: 'Name' and 'Ticket'\")\n\ncols_delete = ['Name', 'Ticket']\n\ntrain = train.drop(cols_delete, axis=1)\ntest = test.drop(cols_delete, axis=1)\n</pre> logger.info(\"Remove variables: 'Name' and 'Ticket'\")  cols_delete = ['Name', 'Ticket']  train = train.drop(cols_delete, axis=1) test = test.drop(cols_delete, axis=1) <pre>2024-06-09 18:03:20.933 | INFO     | __main__:&lt;module&gt;:1 - Remove variables: 'Name' and 'Ticket'\n</pre> In\u00a0[5]: Copied! <pre>logger.info(\"Fill 'Age' with the mean\")\nage_mean = round(train['Age'].mean())\n\ntrain['Age'] = train['Age'].fillna(age_mean)\ntest['Age'] = test['Age'].fillna(age_mean)\n</pre> logger.info(\"Fill 'Age' with the mean\") age_mean = round(train['Age'].mean())  train['Age'] = train['Age'].fillna(age_mean) test['Age'] = test['Age'].fillna(age_mean) <pre>2024-06-09 18:03:20.949 | INFO     | __main__:&lt;module&gt;:1 - Fill 'Age' with the mean\n</pre> In\u00a0[6]: Copied! <pre>logger.info(\"Modify and fill missing values in 'Cabin'\")\ntrain['Cabin'] = train['Cabin'].fillna('N').str[0]\ntest['Cabin'] = test['Cabin'].fillna('N').str[0]\n</pre> logger.info(\"Modify and fill missing values in 'Cabin'\") train['Cabin'] = train['Cabin'].fillna('N').str[0] test['Cabin'] = test['Cabin'].fillna('N').str[0] <pre>2024-06-09 18:03:20.965 | INFO     | __main__:&lt;module&gt;:1 - Modify and fill missing values in 'Cabin'\n</pre> In\u00a0[7]: Copied! <pre>logger.info(\"Change data type: 'Pclass', 'SibSp', and 'Parch'\")\n\ncolumns_to_convert = ['Pclass', 'SibSp', 'Parch']\ntrain[columns_to_convert] = train[columns_to_convert].astype(str)\ntest[columns_to_convert] = test[columns_to_convert].astype(str)\n</pre> logger.info(\"Change data type: 'Pclass', 'SibSp', and 'Parch'\")  columns_to_convert = ['Pclass', 'SibSp', 'Parch'] train[columns_to_convert] = train[columns_to_convert].astype(str) test[columns_to_convert] = test[columns_to_convert].astype(str) <pre>2024-06-09 18:03:20.981 | INFO     | __main__:&lt;module&gt;:1 - Change data type: 'Pclass', 'SibSp', and 'Parch'\n</pre> In\u00a0[8]: Copied! <pre># Display train and test dataset\nlogger.info(\"New train data\")\ntrain.head()\n</pre> # Display train and test dataset logger.info(\"New train data\") train.head() <pre>2024-06-09 18:03:20.996 | INFO     | __main__:&lt;module&gt;:2 - New train data\n</pre> Out[8]: PassengerId Survived Pclass Sex Age SibSp Parch Fare Cabin Embarked 0 1 0 3 male 22.000 1 0 7.250 N S 1 2 1 1 female 38.000 1 0 71.283 C C 2 3 1 3 female 26.000 0 0 7.925 N S 3 4 1 1 female 35.000 1 0 53.100 C S 4 5 0 3 male 35.000 0 0 8.050 N S In\u00a0[9]: Copied! <pre>logger.info(\"New test data\")\ntest.head()\n</pre> logger.info(\"New test data\") test.head() <pre>2024-06-09 18:03:21.011 | INFO     | __main__:&lt;module&gt;:1 - New test data\n</pre> Out[9]: PassengerId Pclass Sex Age SibSp Parch Fare Cabin Embarked 0 892 3 male 34.500 0 0 7.829 N Q 1 893 3 female 47.000 1 0 7.000 N S 2 894 2 male 62.000 0 0 9.688 N Q 3 895 3 male 27.000 0 0 8.662 N S 4 896 3 female 22.000 1 1 12.287 N S In\u00a0[10]: Copied! <pre>logger.info(\"Save Results\")\n\ntrain.to_csv(path_processed + 'train.csv', sep=',', index=False)\ntest.to_csv(path_processed + 'test.csv', sep=',', index=False)\n</pre> logger.info(\"Save Results\")  train.to_csv(path_processed + 'train.csv', sep=',', index=False) test.to_csv(path_processed + 'test.csv', sep=',', index=False) <pre>2024-06-09 18:03:21.031 | INFO     | __main__:&lt;module&gt;:1 - Save Results\n</pre>"},{"location":"project/02_fe/#feature-engineering","title":"\ud83d\udcdd Feature Engineering\u00b6","text":""},{"location":"project/02_fe/#introduction","title":"Introduction\u00b6","text":"<p>Feature engineering is a crucial step in the data preprocessing pipeline, aimed at enhancing the predictive power of machine learning models. For the Titanic dataset, this involves creating new features and modifying existing ones to better capture the underlying patterns that influence passenger survival.</p> <p>The Titanic dataset includes various columns such as 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', and 'Embarked'. Each of these features holds potential insights into the survival outcomes, but they often require transformation and enrichment to become more effective for predictive modeling.</p> <p>Feature engineering is an iterative process that involves experimenting with different transformations and evaluating their impact on model performance. By carefully crafting and selecting features, we can significantly improve the accuracy and robustness of predictive models for the Titanic dataset.</p>"},{"location":"project/02_fe/#feature-engineering","title":"Feature Engineering\u00b6","text":"<p>This stage offers numerous opportunities for a deeper analysis, especially when comparing with other columns. However, for our practical purposes, we will follow this approach:</p> <ul> <li>We will remove the 'Name' and 'Ticket' columns, as they do not initially contribute significantly to the model.</li> <li>For the 'Age' variable, we will fill the missing values with the mean age.</li> <li>We will address the 'Cabin' column by replacing the missing values with the most frequent value, thus optimizing data integrity.</li> <li>We will change the data type of the 'Pclass', 'SibSp', and 'Parch' variables.</li> </ul>"},{"location":"project/02_fe/#conclusion","title":"Conclusion\u00b6","text":"<p>In our feature engineering process for the Titanic dataset, we undertook several steps to prepare the data for effective modeling:</p> <ol> <li>Removal of Non-Contributory Columns: We removed the 'Name' and 'Ticket' columns, as they did not provide significant predictive value for our model.</li> <li>Handling Missing Values:<ul> <li>For the 'Age' column, missing values were filled with the mean age to maintain consistency and avoid data loss.</li> <li>For the 'Cabin' column, missing values were replaced with the most frequent value ('N'), and only the first letter of the cabin was retained to simplify the data.</li> </ul> </li> <li>Data Type Conversion: The columns 'Pclass', 'SibSp', and 'Parch' were converted from numerical to string type to better capture categorical relationships.</li> <li>Data Saving: The processed training and test datasets were saved for future modeling and analysis.</li> </ol> <p>These feature engineering steps have improved the quality and usability of the dataset, ensuring that it is well-prepared for subsequent analysis and machine learning tasks. By addressing missing values, simplifying categorical data, and removing unnecessary columns, we have created a more robust and interpretable dataset for predicting passenger survival on the Titanic.</p>"},{"location":"project/03_ml/","title":"\ud83e\udd16 Machine Learning","text":"In\u00a0[1]: Copied! <pre># Libraries\nfrom loguru import logger\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nimport joblib\nimport time\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_curve,\n    roc_auc_score,\n)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n</pre> # Libraries from loguru import logger import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  import warnings warnings.filterwarnings(\"ignore\") pd.set_option('display.max_columns', 500) pd.set_option('display.max_rows', 500) pd.set_option('display.float_format', lambda x: '%.3f' % x)  import joblib import time  from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier from sklearn.svm import SVC from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import AdaBoostClassifier from lightgbm import LGBMClassifier  from sklearn.metrics import (     accuracy_score,     precision_score,     recall_score,     f1_score,     roc_curve,     roc_auc_score, )  from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer In\u00a0[2]: Copied! <pre>def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n    start_time = time.time()\n    model.fit(X_train, y_train)\n    execution_time = time.time() - start_time\n\n    y_pred = model.predict(X_test)\n    accuracy = round(accuracy_score(y_test, y_pred), 3)\n    precision = round(precision_score(y_test, y_pred), 3)\n    recall = round(recall_score(y_test, y_pred), 3)\n    f1 = round(f1_score(y_test, y_pred), 3)\n\n    y_prob = model.predict_proba(X_test)[:, 1]\n    fpr, tpr, _ = roc_curve(y_test, y_prob)\n    auc = round(roc_auc_score(y_test, y_prob), 3)\n\n    evaluation_metrics = {\n        \"Accuracy\": accuracy,\n        \"Precision\": precision,\n        \"Recall\": recall,\n        \"F1-Score\": f1,\n        \"AUC\": auc,\n        \"Time\": round(execution_time, 3),\n    }\n\n    return evaluation_metrics\n\n\n\ndef train_and_evaluate_all_models(models_dict, X_train, y_train, X_test, y_test):\n    evaluation_results = {}\n    for model_name, model in models_dict.items():\n        evaluation_metrics = train_and_evaluate_model(\n            model, X_train, y_train, X_test, y_test\n        )\n        evaluation_results[model_name] = evaluation_metrics\n\n    results_df = pd.DataFrame.from_dict(evaluation_results, orient=\"index\")\n    return results_df\n\ndef preprocess_applier(preprocessor, X_data):\n    # Apply preprocessing to the data\n    X_data_processed = preprocessor.transform(X_data)\n\n    # Get column names after preprocessing\n    numeric_feature_names = preprocessor.transformers_[0][-1]\n    categorical_feature_names = preprocessor.transformers_[1][-1]\n\n    # Get the unique categories of the categorical variables\n    unique_categories = preprocessor.named_transformers_[\"cat\"][\"onehot\"].categories_\n\n    # Create column names after OneHotEncoding\n    encoded_categorical_feature_names = []\n    for i, categories in enumerate(unique_categories):\n        for category in categories:\n            encoded_categorical_feature_names.append(\n                f\"{categorical_feature_names[i]}_{category}\"\n            )\n\n    # Convert the sparse matrix to a Pandas DataFrame\n    transformed_df = pd.DataFrame(\n        X_data_processed.toarray(),\n        columns=numeric_feature_names + encoded_categorical_feature_names,\n    )\n\n    return transformed_df\n</pre> def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):     start_time = time.time()     model.fit(X_train, y_train)     execution_time = time.time() - start_time      y_pred = model.predict(X_test)     accuracy = round(accuracy_score(y_test, y_pred), 3)     precision = round(precision_score(y_test, y_pred), 3)     recall = round(recall_score(y_test, y_pred), 3)     f1 = round(f1_score(y_test, y_pred), 3)      y_prob = model.predict_proba(X_test)[:, 1]     fpr, tpr, _ = roc_curve(y_test, y_prob)     auc = round(roc_auc_score(y_test, y_prob), 3)      evaluation_metrics = {         \"Accuracy\": accuracy,         \"Precision\": precision,         \"Recall\": recall,         \"F1-Score\": f1,         \"AUC\": auc,         \"Time\": round(execution_time, 3),     }      return evaluation_metrics    def train_and_evaluate_all_models(models_dict, X_train, y_train, X_test, y_test):     evaluation_results = {}     for model_name, model in models_dict.items():         evaluation_metrics = train_and_evaluate_model(             model, X_train, y_train, X_test, y_test         )         evaluation_results[model_name] = evaluation_metrics      results_df = pd.DataFrame.from_dict(evaluation_results, orient=\"index\")     return results_df  def preprocess_applier(preprocessor, X_data):     # Apply preprocessing to the data     X_data_processed = preprocessor.transform(X_data)      # Get column names after preprocessing     numeric_feature_names = preprocessor.transformers_[0][-1]     categorical_feature_names = preprocessor.transformers_[1][-1]      # Get the unique categories of the categorical variables     unique_categories = preprocessor.named_transformers_[\"cat\"][\"onehot\"].categories_      # Create column names after OneHotEncoding     encoded_categorical_feature_names = []     for i, categories in enumerate(unique_categories):         for category in categories:             encoded_categorical_feature_names.append(                 f\"{categorical_feature_names[i]}_{category}\"             )      # Convert the sparse matrix to a Pandas DataFrame     transformed_df = pd.DataFrame(         X_data_processed.toarray(),         columns=numeric_feature_names + encoded_categorical_feature_names,     )      return transformed_df In\u00a0[3]: Copied! <pre>logger.info(\"Read Data\")\n\n# Paths\npath_raw = \"../../data/raw/\"\npath_processed = \"../../data/processed/\"\npath_final = \"../../data/final/\"\n\n# Read data\ntrain = pd.read_csv(path_processed + \"train.csv\")\ntest = pd.read_csv(path_processed + \"test.csv\")\n\ncolumns_to_convert = ['Pclass', 'SibSp', 'Parch']\ntrain[columns_to_convert] = train[columns_to_convert].astype(str)\ntest[columns_to_convert] = test[columns_to_convert].astype(str)\n\n# Get column names by data types\ntarget_variable = 'Survived'\n\nfloat_columns = [x for x in list(train.select_dtypes(include=['float64']).columns) if x != target_variable]\ninteger_columns = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x != target_variable]\nobject_columns = [x for x in list(train.select_dtypes(include=['object']).columns) if x != target_variable]\n</pre> logger.info(\"Read Data\")  # Paths path_raw = \"../../data/raw/\" path_processed = \"../../data/processed/\" path_final = \"../../data/final/\"  # Read data train = pd.read_csv(path_processed + \"train.csv\") test = pd.read_csv(path_processed + \"test.csv\")  columns_to_convert = ['Pclass', 'SibSp', 'Parch'] train[columns_to_convert] = train[columns_to_convert].astype(str) test[columns_to_convert] = test[columns_to_convert].astype(str)  # Get column names by data types target_variable = 'Survived'  float_columns = [x for x in list(train.select_dtypes(include=['float64']).columns) if x != target_variable] integer_columns = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x != target_variable] object_columns = [x for x in list(train.select_dtypes(include=['object']).columns) if x != target_variable] <pre>2024-06-09 20:11:17.992 | INFO     | __main__:&lt;module&gt;:1 - Read Data\n</pre> In\u00a0[4]: Copied! <pre>logger.info(\"Split the dataset into training and testing sets\")\n\n# Split the dataset into training and testing sets\ntarget = 'Survived'\nindex_column = 'PassengerId'\n\nfeatures = [x for x in train.columns if x not in [target, index_column]]\n\nX = train[features]\ny = train[target]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Example of numeric and categorical variables\nnumeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object']).columns.tolist()\n\n# Create transformers for numeric and categorical variables\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Create the ColumnTransformer to apply transformations in a pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Apply preprocessing to the training and testing data\nX_train_processed = preprocessor.fit_transform(X_train)\nX_test_processed = preprocessor.transform(X_test)\n\n# Get column names after preprocessing\nnumeric_feature_names = preprocessor.transformers_[0][-1]\ncategorical_feature_names = preprocessor.transformers_[1][-1]\n\n# Get the unique categories of the categorical variables\nunique_categories = preprocessor.named_transformers_['cat']['onehot'].categories_\n\n# Create column names after OneHotEncoding\nencoded_categorical_feature_names = []\nfor i, categories in enumerate(unique_categories):\n    for category in categories:\n        encoded_categorical_feature_names.append(f'{categorical_feature_names[i]}_{category}')\n\n# Convert the sparse matrix to a Pandas DataFrame\ntransformed_train_df = pd.DataFrame(X_train_processed.toarray(), columns=numeric_feature_names + encoded_categorical_feature_names)\ntransformed_test_df = pd.DataFrame(X_test_processed.toarray(), columns=numeric_feature_names + encoded_categorical_feature_names)\n</pre> logger.info(\"Split the dataset into training and testing sets\")  # Split the dataset into training and testing sets target = 'Survived' index_column = 'PassengerId'  features = [x for x in train.columns if x not in [target, index_column]]  X = train[features] y = train[target]  # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Example of numeric and categorical variables numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist() categorical_features = X.select_dtypes(include=['object']).columns.tolist()  # Create transformers for numeric and categorical variables numeric_transformer = Pipeline(steps=[     ('imputer', SimpleImputer(strategy='median')),     ('scaler', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[     ('imputer', SimpleImputer(strategy='most_frequent')),     ('onehot', OneHotEncoder(handle_unknown='ignore')) ])  # Create the ColumnTransformer to apply transformations in a pipeline preprocessor = ColumnTransformer(     transformers=[         ('num', numeric_transformer, numeric_features),         ('cat', categorical_transformer, categorical_features)     ])  # Apply preprocessing to the training and testing data X_train_processed = preprocessor.fit_transform(X_train) X_test_processed = preprocessor.transform(X_test)  # Get column names after preprocessing numeric_feature_names = preprocessor.transformers_[0][-1] categorical_feature_names = preprocessor.transformers_[1][-1]  # Get the unique categories of the categorical variables unique_categories = preprocessor.named_transformers_['cat']['onehot'].categories_  # Create column names after OneHotEncoding encoded_categorical_feature_names = [] for i, categories in enumerate(unique_categories):     for category in categories:         encoded_categorical_feature_names.append(f'{categorical_feature_names[i]}_{category}')  # Convert the sparse matrix to a Pandas DataFrame transformed_train_df = pd.DataFrame(X_train_processed.toarray(), columns=numeric_feature_names + encoded_categorical_feature_names) transformed_test_df = pd.DataFrame(X_test_processed.toarray(), columns=numeric_feature_names + encoded_categorical_feature_names) <pre>2024-06-09 20:11:18.024 | INFO     | __main__:&lt;module&gt;:1 - Split the dataset into training and testing sets\n</pre> In\u00a0[5]: Copied! <pre># Models with optimized hyperparameters\n\n# Initialize RandomForest classifier with optimized hyperparameters\nrandom_forest = RandomForestClassifier(random_state=42,\n                                       n_estimators=100,\n                                       max_depth=None,\n                                       min_samples_split=2,\n                                       min_samples_leaf=1)\n\n# Initialize LGBM classifier with optimized hyperparameters\nlgbm = LGBMClassifier(random_state=42,\n                      n_estimators=100,\n                      learning_rate=0.1,\n                      max_depth=-1)\n\n# Initialize DecisionTree classifier with optimized hyperparameters\ndecision_tree = DecisionTreeClassifier(random_state=42,\n                                       max_depth=None,\n                                       min_samples_split=2,\n                                       min_samples_leaf=1)\n\n# Initialize KNeighbors classifier with optimized hyperparameters\nknn = KNeighborsClassifier(n_neighbors=5,\n                           weights='uniform',\n                           p=2)\n\n# Initialize LogisticRegression classifier with optimized hyperparameters\nlogistic_regression = LogisticRegression(random_state=42,\n                                         C=1.0,\n                                         penalty='l2')\n\n# Initialize GaussianNB classifier with optimized hyperparameters\ngaussian_nb = GaussianNB(var_smoothing=1e-9)\n\n# Initialize AdaBoost classifier with optimized hyperparameters\nada_boost = AdaBoostClassifier(random_state=42,\n                               n_estimators=50,\n                               learning_rate=0.1)\n\n# Create a dictionary of models with their parameters for easy iteration\nmodels = {\n    'Random Forest': random_forest, \n    'LGBM': lgbm,\n    'Decision Tree': decision_tree,\n    'KNN': knn, \n    'Logistic Regression': logistic_regression,\n    'GaussianNB': gaussian_nb,\n    'AdaBoost': ada_boost\n}\n</pre> # Models with optimized hyperparameters  # Initialize RandomForest classifier with optimized hyperparameters random_forest = RandomForestClassifier(random_state=42,                                        n_estimators=100,                                        max_depth=None,                                        min_samples_split=2,                                        min_samples_leaf=1)  # Initialize LGBM classifier with optimized hyperparameters lgbm = LGBMClassifier(random_state=42,                       n_estimators=100,                       learning_rate=0.1,                       max_depth=-1)  # Initialize DecisionTree classifier with optimized hyperparameters decision_tree = DecisionTreeClassifier(random_state=42,                                        max_depth=None,                                        min_samples_split=2,                                        min_samples_leaf=1)  # Initialize KNeighbors classifier with optimized hyperparameters knn = KNeighborsClassifier(n_neighbors=5,                            weights='uniform',                            p=2)  # Initialize LogisticRegression classifier with optimized hyperparameters logistic_regression = LogisticRegression(random_state=42,                                          C=1.0,                                          penalty='l2')  # Initialize GaussianNB classifier with optimized hyperparameters gaussian_nb = GaussianNB(var_smoothing=1e-9)  # Initialize AdaBoost classifier with optimized hyperparameters ada_boost = AdaBoostClassifier(random_state=42,                                n_estimators=50,                                learning_rate=0.1)  # Create a dictionary of models with their parameters for easy iteration models = {     'Random Forest': random_forest,      'LGBM': lgbm,     'Decision Tree': decision_tree,     'KNN': knn,      'Logistic Regression': logistic_regression,     'GaussianNB': gaussian_nb,     'AdaBoost': ada_boost } In\u00a0[6]: Copied! <pre># Call the function to train and evaluate all models\nlogger.info(\"Train and evaluate all models\")\nresults_df = train_and_evaluate_all_models(models, transformed_train_df, y_train, transformed_test_df, y_test)\n</pre> # Call the function to train and evaluate all models logger.info(\"Train and evaluate all models\") results_df = train_and_evaluate_all_models(models, transformed_train_df, y_train, transformed_test_df, y_test) <pre>2024-06-09 20:11:18.067 | INFO     | __main__:&lt;module&gt;:2 - Train and evaluate all models\n</pre> <pre>[LightGBM] [Info] Number of positive: 268, number of negative: 444\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000384 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 208\n[LightGBM] [Info] Number of data points in the train set: 712, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.376404 -&gt; initscore=-0.504838\n[LightGBM] [Info] Start training from score -0.504838\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n</pre> In\u00a0[7]: Copied! <pre># Display the DataFrame with the results\nlogger.info(\"Sort the results by the AUC metric\")\nresults_df.sort_values('AUC', ascending=False)\n</pre> # Display the DataFrame with the results logger.info(\"Sort the results by the AUC metric\") results_df.sort_values('AUC', ascending=False) <pre>2024-06-09 20:11:18.474 | INFO     | __main__:&lt;module&gt;:2 - Sort the results by the AUC metric\n</pre> Out[7]: Accuracy Precision Recall F1-Score AUC Time Random Forest 0.810 0.794 0.730 0.761 0.886 0.098 Logistic Regression 0.810 0.786 0.743 0.764 0.875 0.016 KNN 0.821 0.809 0.743 0.775 0.872 0.001 LGBM 0.793 0.768 0.716 0.741 0.871 0.145 AdaBoost 0.788 0.757 0.716 0.736 0.866 0.053 Decision Tree 0.788 0.743 0.743 0.743 0.788 0.004 GaussianNB 0.419 0.415 0.986 0.584 0.778 0.001 <p>Model Performance Evaluation Results</p> <p>The table below presents the performance metrics for various machine learning models applied to the Titanic dataset. The metrics include Accuracy, Precision, Recall, F1-Score, AUC (Area Under the ROC Curve), and the Time taken for training and evaluation. Each metric provides insights into different aspects of model performance.</p> <p>Explanation of Metrics:</p> <ul> <li>Accuracy: The proportion of correctly classified instances among the total instances. A higher value indicates better overall performance.</li> <li>Precision: The proportion of true positive predictions among all positive predictions. It reflects the model's ability to avoid false positives.</li> <li>Recall: The proportion of true positive predictions among all actual positives. It indicates the model's ability to capture all relevant instances (sensitivity).</li> <li>F1-Score: The harmonic mean of precision and recall, providing a balance between the two. It is particularly useful when the class distribution is imbalanced.</li> <li>AUC (Area Under the ROC Curve): Measures the model's ability to distinguish between classes. A higher AUC value indicates better performance.</li> <li>Time: The time taken to train and evaluate the model.</li> </ul> <p>Model Insights:</p> <ol> <li>Random Forest: Achieved a high AUC of 0.886, indicating excellent discrimination between classes, with a good balance of precision and recall.</li> <li>Logistic Regression: Performed similarly to Random Forest with high accuracy and a strong F1-Score, but slightly lower AUC.</li> <li>KNN: Showed the highest accuracy and a strong F1-Score, but with a marginally lower AUC compared to Random Forest and Logistic Regression.</li> <li>LGBM: Performed well but with a slightly lower accuracy and AUC than Random Forest, Logistic Regression, and KNN.</li> <li>AdaBoost: Had decent performance but was slightly less effective in terms of precision and AUC compared to the top models.</li> <li>Decision Tree: Demonstrated good precision and recall but with a significantly lower AUC.</li> <li>GaussianNB: Had the lowest performance metrics, indicating poor model performance, especially with a very high recall but low precision and AUC.</li> </ol> <p>Conclusion:</p> <p>Among the models evaluated, the Random Forest, Logistic Regression, and KNN classifiers showed the best overall performance, with high accuracy, precision, recall, F1-Score, and AUC values. Random Forest had the highest AUC, making it the best model for distinguishing between classes. Logistic Regression and KNN also performed well, with KNN achieving the highest accuracy. The time metric indicates that Logistic Regression is the fastest to train and evaluate, followed by KNN, making them efficient choices for quick model training. GaussianNB showed the poorest performance, highlighting its unsuitability for this specific classification task.</p> In\u00a0[8]: Copied! <pre>logger.info(\"Select model\")\n\nmodel = RandomForestClassifier(random_state=42)\n\nmodel.fit(transformed_train_df, y_train)\n</pre> logger.info(\"Select model\")  model = RandomForestClassifier(random_state=42)  model.fit(transformed_train_df, y_train) <pre>2024-06-09 20:11:18.489 | INFO     | __main__:&lt;module&gt;:1 - Select model\n</pre> Out[8]: <pre>RandomForestClassifier(random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0RandomForestClassifier?Documentation for RandomForestClassifieriFitted<pre>RandomForestClassifier(random_state=42)</pre> In\u00a0[9]: Copied! <pre># Get hyperparameters\nhyperparameters = model.get_params()\nprint(hyperparameters)\n</pre> # Get hyperparameters hyperparameters = model.get_params() print(hyperparameters) <pre>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n</pre> In\u00a0[10]: Copied! <pre># Get feature importance\nfeature_importance = model.feature_importances_\n\n# Get feature names\nfeature_names = transformed_train_df.columns\n\n# Create a DataFrame with features and their importance\nfeature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n\n# Sort the DataFrame by importance in descending order\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n# Display the most important features\nfeature_importance_df.head(10)\n</pre> # Get feature importance feature_importance = model.feature_importances_  # Get feature names feature_names = transformed_train_df.columns  # Create a DataFrame with features and their importance feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})  # Sort the DataFrame by importance in descending order feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)  # Display the most important features feature_importance_df.head(10) Out[10]: Feature Importance 0 Age 0.220 1 Fare 0.220 5 Sex_female 0.135 6 Sex_male 0.134 4 Pclass_3 0.044 28 Cabin_N 0.032 3 Pclass_2 0.020 2 Pclass_1 0.020 8 SibSp_1 0.017 32 Embarked_S 0.017 In\u00a0[11]: Copied! <pre># Bar plot to visualize the most important features\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance_df['Feature'][:10], feature_importance_df['Importance'][:10], color='#bce4b5')\nplt.xlabel('Importance')\nplt.title('Top 10 Feature Importance')\nplt.gca().invert_yaxis()  # Invert the y-axis so the most important features are at the top\nplt.show()\n</pre> # Bar plot to visualize the most important features plt.figure(figsize=(10, 6)) plt.barh(feature_importance_df['Feature'][:10], feature_importance_df['Importance'][:10], color='#bce4b5') plt.xlabel('Importance') plt.title('Top 10 Feature Importance') plt.gca().invert_yaxis()  # Invert the y-axis so the most important features are at the top plt.show() In\u00a0[12]: Copied! <pre>logger.info(\"Make predictions\")\n\nX_test_processed2 = preprocess_applier(preprocessor, test.drop('PassengerId', axis=1))\npredictions = model.predict(X_test_processed2)\ntest[\"Survived\"] = predictions\ntest.head()\n</pre> logger.info(\"Make predictions\")  X_test_processed2 = preprocess_applier(preprocessor, test.drop('PassengerId', axis=1)) predictions = model.predict(X_test_processed2) test[\"Survived\"] = predictions test.head() <pre>2024-06-09 20:11:18.741 | INFO     | __main__:&lt;module&gt;:1 - Make predictions\n</pre> Out[12]: PassengerId Pclass Sex Age SibSp Parch Fare Cabin Embarked Survived 0 892 3 male 34.500 0 0 7.829 N Q 0 1 893 3 female 47.000 1 0 7.000 N S 0 2 894 2 male 62.000 0 0 9.688 N Q 0 3 895 3 male 27.000 0 0 8.662 N S 1 4 896 3 female 22.000 1 1 12.287 N S 0 In\u00a0[13]: Copied! <pre>logger.info(\"Save Results\")\n\nlogger.info(\"Save prediction results\")\ntest.to_csv(path_final + \"predictions.csv\", index=False, sep=',')\n\nlogger.info(\"Save results of the different models\")\nresults_df.to_csv(path_final + \"models_metrics.csv\", index=False, sep=',')\n</pre> logger.info(\"Save Results\")  logger.info(\"Save prediction results\") test.to_csv(path_final + \"predictions.csv\", index=False, sep=',')  logger.info(\"Save results of the different models\") results_df.to_csv(path_final + \"models_metrics.csv\", index=False, sep=',') <pre>2024-06-09 20:11:18.773 | INFO     | __main__:&lt;module&gt;:1 - Save Results\n2024-06-09 20:11:18.774 | INFO     | __main__:&lt;module&gt;:3 - Save prediction results\n2024-06-09 20:11:18.777 | INFO     | __main__:&lt;module&gt;:6 - Save results of the different models\n</pre>"},{"location":"project/03_ml/#machine-learning","title":"\ud83e\udd16 Machine Learning\u00b6","text":""},{"location":"project/03_ml/#introduction","title":"Introduction\u00b6","text":"<p>The Titanic dataset is a popular and classic dataset used for introducing machine learning concepts and techniques. This dataset contains information about the passengers aboard the Titanic, including features such as age, gender, ticket class, and whether or not they survived the disaster. The primary objective is to build a predictive model that can accurately classify whether a passenger survived or not based on these features.</p> <p>Machine learning offers a range of algorithms that can be applied to this classification problem. These algorithms can be broadly categorized into supervised learning techniques, where the model is trained on a labeled dataset. For the Titanic dataset, this means using the known outcomes (survived or not) to train the model.</p> <p>Key steps in applying machine learning to the Titanic dataset include:</p> <ol> <li><p>Data Preprocessing: This involves cleaning the data, handling missing values, and performing feature engineering to create relevant features that will improve the model's performance. The preprocessing steps ensure that the data is in a suitable format for training.</p> </li> <li><p>Splitting the Data: The dataset is typically split into a training set and a test set. The training set is used to train the model, while the test set is used to evaluate its performance.</p> </li> <li><p>Selecting and Training Models: Various machine learning algorithms can be applied to the Titanic dataset, including:</p> <ul> <li>Logistic Regression: A simple and interpretable algorithm suitable for binary classification problems.</li> <li>Decision Trees: A non-linear model that captures complex interactions between features.</li> <li>Random Forests: An ensemble method that builds multiple decision trees and combines their predictions for improved accuracy.</li> <li>Support Vector Machines (SVM): A powerful classifier that can find the optimal boundary between classes.</li> <li>Gradient Boosting: An ensemble technique that builds models sequentially to correct errors made by previous models.</li> </ul> </li> <li><p>Model Evaluation: The performance of the models is evaluated using metrics such as accuracy, precision, recall, and the F1 score. Cross-validation techniques can also be employed to ensure the model's robustness and to prevent overfitting.</p> </li> <li><p>Hyperparameter Tuning: This involves optimizing the parameters of the chosen algorithms to improve their performance. Techniques like grid search or random search can be used for this purpose.</p> </li> <li><p>Making Predictions: Once the model is trained and evaluated, it can be used to make predictions on new, unseen data. In the case of the Titanic dataset, this would involve predicting the survival of passengers based on their features.</p> </li> </ol> <p>By applying machine learning techniques to the Titanic dataset, we can gain valuable insights into the factors that influenced survival and develop predictive models that can be used for similar classification tasks in other domains. The process also provides a practical introduction to key machine learning concepts and methods.</p>"},{"location":"project/03_ml/#apply-machine-learning-models","title":"Apply Machine Learning Models\u00b6","text":"<p>In this section, we will apply various machine learning models to the Titanic dataset to predict passenger survival. By leveraging algorithms such as Logistic Regression, Decision Trees, Random Forests, Support Vector Machines (SVM), and Gradient Boosting, we aim to build and evaluate predictive models. These models will help us understand the key factors influencing survival and demonstrate the application of machine learning techniques to real-world data.</p>"},{"location":"project/03_ml/#utilizing-the-best-model","title":"Utilizing the Best Model\u00b6","text":"<p>In this section, we will focus on leveraging the best-performing machine learning model identified during our evaluation phase. Based on our performance metrics, the Random Forest classifier demonstrated superior results with the highest AUC and a strong balance of precision and recall. We will now utilize this model to make predictions on new data and explore its potential applications. This includes deploying the model, interpreting its predictions, and assessing its real-world impact. By harnessing the power of the best model, we aim to achieve accurate and actionable insights from the Titanic dataset.</p>"},{"location":"project/03_ml/#conclusion","title":"Conclusion\u00b6","text":"<p>Throughout the machine learning phase of our Titanic dataset analysis, we successfully built and evaluated several predictive models to determine passenger survival. The process involved several critical steps:</p> <ol> <li><p>Data Preprocessing: We prepared the data by handling missing values, encoding categorical variables, and scaling numerical features. This ensured that our data was clean and suitable for model training.</p> </li> <li><p>Model Selection and Training: We experimented with various machine learning algorithms, including Random Forest, Logistic Regression, K-Nearest Neighbors (KNN), LightGBM, AdaBoost, Decision Tree, and Gaussian Naive Bayes. Each model was trained using optimized hyperparameters to enhance performance.</p> </li> <li><p>Evaluation Metrics: We evaluated the models based on key metrics such as Accuracy, Precision, Recall, F1-Score, and AUC (Area Under the ROC Curve). This comprehensive evaluation allowed us to identify the strengths and weaknesses of each model.</p> </li> <li><p>Best Model Identification: Among the models, the Random Forest classifier emerged as the best-performing model with the highest AUC and a strong balance of precision and recall. This model demonstrated superior ability to distinguish between passengers who survived and those who did not.</p> </li> <li><p>Feature Importance: Using the Random Forest model, we identified the most important features contributing to the prediction of survival. This insight helps in understanding the factors that significantly influenced the survival chances of passengers.</p> </li> <li><p>Predictions: We utilized the best-performing model to make predictions on the test dataset, providing an actionable outcome based on our analysis.</p> </li> <li><p>Results Documentation: Finally, we saved the prediction results and the evaluation metrics of all models. This documentation ensures reproducibility and allows for further analysis and refinement.</p> </li> </ol> <p>Overall, the machine learning phase has provided us with valuable predictive insights and a robust model for assessing passenger survival on the Titanic. The comprehensive approach, from data preprocessing to model evaluation and deployment, underscores the importance of methodical and thorough analysis in achieving accurate and meaningful results.</p>"},{"location":"project/utils/","title":"Utils","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> import pandas as pd import matplotlib.pyplot as plt import seaborn as sns In\u00a0[\u00a0]: Copied! <pre>def plot_histogram(data, column, figsize=(8, 4)):\n    \"\"\"\n    Creates and displays a histogram of the distribution of the data for a given column in a DataFrame.\n\n    Parameters:\n    data (pandas DataFrame): The DataFrame containing the data.\n    column (str): The name of the column for which the histogram will be created.\n    figsize (tuple, optional): The size of the histogram figure. Default is (8, 4).\n\n    Returns:\n    None\n    \"\"\"\n\n    # Configure the Seaborn style\n    sns.set(style='whitegrid')\n\n    # Create a figure with the specified size\n    plt.figure(figsize=figsize)\n\n    # Create the histogram using Seaborn\n    sns.histplot(data[column].dropna(), bins=20, color='#bce4b5', edgecolor='black')\n\n    # Add title and axis labels\n    plt.title(f'Distribution of column {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n\n    # Show the grid in the plot\n    plt.grid(True)\n\n    # Display the histogram\n    plt.show()\n</pre> def plot_histogram(data, column, figsize=(8, 4)):     \"\"\"     Creates and displays a histogram of the distribution of the data for a given column in a DataFrame.      Parameters:     data (pandas DataFrame): The DataFrame containing the data.     column (str): The name of the column for which the histogram will be created.     figsize (tuple, optional): The size of the histogram figure. Default is (8, 4).      Returns:     None     \"\"\"      # Configure the Seaborn style     sns.set(style='whitegrid')      # Create a figure with the specified size     plt.figure(figsize=figsize)      # Create the histogram using Seaborn     sns.histplot(data[column].dropna(), bins=20, color='#bce4b5', edgecolor='black')      # Add title and axis labels     plt.title(f'Distribution of column {column}')     plt.xlabel(column)     plt.ylabel('Frequency')      # Show the grid in the plot     plt.grid(True)      # Display the histogram     plt.show() In\u00a0[\u00a0]: Copied! <pre>def plot_histogram_vo(data, column, vo, figsize=(8, 4)):\n    \"\"\"\n    Creates and displays a histogram of the distribution of the data for a given column in a DataFrame,\n    divided by a categorical variable (target variable).\n\n    Parameters:\n    data (pandas DataFrame): The DataFrame containing the data.\n    column (str): The name of the column for which the histogram will be created.\n    vo (str): The name of the target variable to divide the data in the histogram.\n    figsize (tuple, optional): The size of the histogram figure. Default is (8, 4).\n\n    Returns:\n    None\n    \"\"\"\n\n    # Configure the Seaborn style\n    sns.set(style='whitegrid')\n\n    # Create a figure with the specified size\n    plt.figure(figsize=figsize)\n\n    # Create histograms using Seaborn\n    sns.histplot(x=column, hue=vo, data=data, palette='Greens', edgecolor='black')\n\n    # Add title and axis labels\n    plt.title(f'Distribution of column {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n\n    # Display the histogram\n    plt.show()\n</pre> def plot_histogram_vo(data, column, vo, figsize=(8, 4)):     \"\"\"     Creates and displays a histogram of the distribution of the data for a given column in a DataFrame,     divided by a categorical variable (target variable).      Parameters:     data (pandas DataFrame): The DataFrame containing the data.     column (str): The name of the column for which the histogram will be created.     vo (str): The name of the target variable to divide the data in the histogram.     figsize (tuple, optional): The size of the histogram figure. Default is (8, 4).      Returns:     None     \"\"\"      # Configure the Seaborn style     sns.set(style='whitegrid')      # Create a figure with the specified size     plt.figure(figsize=figsize)      # Create histograms using Seaborn     sns.histplot(x=column, hue=vo, data=data, palette='Greens', edgecolor='black')      # Add title and axis labels     plt.title(f'Distribution of column {column}')     plt.xlabel(column)     plt.ylabel('Frequency')      # Display the histogram     plt.show() In\u00a0[\u00a0]: Copied! <pre>def plot_range_distribution(data, column, bins, figsize=(8, 4)):\n    \"\"\"\n    Creates and displays a bar chart that represents the distribution of a column\n    divided into specific ranges in a DataFrame.\n\n    Parameters:\n    data (pandas DataFrame): The DataFrame containing the data.\n    column (str): The name of the column for which the range distribution will be created.\n    bins (int or sequence of scalars): The number of bins or bin edges for the division.\n    figsize (tuple, optional): The size of the bar chart figure. Default is (8, 4).\n\n    Returns:\n    None\n    \"\"\"\n\n    # Configure the Seaborn style\n    sns.set(style='whitegrid')\n\n    # Add a new column to the DataFrame with the ranges of the specific column\n    data[column + 'Range'] = pd.cut(data[column], bins=bins, right=False)\n\n    # Count the number of elements in each range\n    temp_counts = data[column + 'Range'].value_counts().sort_index()\n\n    # Calculate the percentages instead of counting the number of elements\n    temp_percentages = (temp_counts / temp_counts.sum()) * 100  # Calculate the relative percentages\n\n    # Create the bar chart\n    plt.figure(figsize=figsize)\n    sns.barplot(x=temp_percentages.index, y=temp_percentages.values, color='#bce4b5', edgecolor='black')\n\n    # Add value annotations on the bars (percentages)\n    for i, value in enumerate(temp_percentages):\n        plt.text(i, value + 0.2, f'{value:.2f}%', ha='center', va='bottom', fontsize=9)\n\n    # Set title and axis labels\n    plt.title(f'Range Distribution of column {column}')\n    plt.xlabel('Ranges')\n    plt.ylabel('Frequency')\n    plt.xticks(rotation=0)  # Rotate x-axis labels for better readability\n    plt.tight_layout()\n\n    # Display the chart\n    plt.show()\n\n    # Remove extra column\n    data.drop(column + 'Range', axis=1, inplace=True)\n</pre> def plot_range_distribution(data, column, bins, figsize=(8, 4)):     \"\"\"     Creates and displays a bar chart that represents the distribution of a column     divided into specific ranges in a DataFrame.      Parameters:     data (pandas DataFrame): The DataFrame containing the data.     column (str): The name of the column for which the range distribution will be created.     bins (int or sequence of scalars): The number of bins or bin edges for the division.     figsize (tuple, optional): The size of the bar chart figure. Default is (8, 4).      Returns:     None     \"\"\"      # Configure the Seaborn style     sns.set(style='whitegrid')      # Add a new column to the DataFrame with the ranges of the specific column     data[column + 'Range'] = pd.cut(data[column], bins=bins, right=False)      # Count the number of elements in each range     temp_counts = data[column + 'Range'].value_counts().sort_index()      # Calculate the percentages instead of counting the number of elements     temp_percentages = (temp_counts / temp_counts.sum()) * 100  # Calculate the relative percentages      # Create the bar chart     plt.figure(figsize=figsize)     sns.barplot(x=temp_percentages.index, y=temp_percentages.values, color='#bce4b5', edgecolor='black')      # Add value annotations on the bars (percentages)     for i, value in enumerate(temp_percentages):         plt.text(i, value + 0.2, f'{value:.2f}%', ha='center', va='bottom', fontsize=9)      # Set title and axis labels     plt.title(f'Range Distribution of column {column}')     plt.xlabel('Ranges')     plt.ylabel('Frequency')     plt.xticks(rotation=0)  # Rotate x-axis labels for better readability     plt.tight_layout()      # Display the chart     plt.show()      # Remove extra column     data.drop(column + 'Range', axis=1, inplace=True) In\u00a0[\u00a0]: Copied! <pre>def plot_range_distribution_vo(data, column, bins, vo, figsize=(8, 4)):\n    \"\"\"\n    Creates and displays a bar chart that represents the distribution of a column divided into specific ranges in a DataFrame,\n    grouped by a target variable and visualizing the relative percentages in each group.\n\n    Parameters:\n    data (pandas DataFrame): The DataFrame containing the data.\n    column (str): The name of the column for which the range distribution will be created.\n    bins (int or sequence of scalars): The number of bins or bin edges for the division.\n    vo (str): The name of the target variable to group the data in the bar chart.\n    figsize (tuple, optional): The size of the bar chart figure. Default is (8, 4).\n\n    Returns:\n    None\n    \"\"\"\n\n    # Configure the Seaborn style\n    sns.set(style='whitegrid')\n\n    # Add a new column to the DataFrame with the ranges of the specific column\n    data[column + 'Range'] = pd.cut(data[column], bins=bins, right=False)\n\n    # Calculate the count of each group and restructure the data\n    counts = data.groupby([vo, column + 'Range']).size().reset_index(name='Count')\n\n    # Calculate the percentages by category\n    counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))\n\n    # Bar chart with Seaborn\n    plt.figure(figsize=figsize)\n    ax = sns.barplot(data=counts, x=column + 'Range', y='Percentage', hue=vo, palette='Greens', edgecolor='black')\n\n    # Rotate the x-axis labels (45 degrees) and add values on each bar (excluding 0%)\n    for p in ax.patches:\n        if p.get_height() != 0:  # If the value is not 0%\n            ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center',\n                        va='center', xytext=(0, 10), textcoords='offset points', fontsize=8)\n\n    # Set title and axis labels\n    plt.title(f'Range Distribution of column {column}')\n    plt.xlabel('Ranges')\n    plt.ylabel('Frequency')\n    plt.xticks(rotation=0)  # Rotate x-axis labels for better readability\n    plt.tight_layout()\n\n    # Display the chart\n    plt.show()\n\n    # Remove extra column\n    data.drop(column + 'Range', axis=1, inplace=True)\n</pre> def plot_range_distribution_vo(data, column, bins, vo, figsize=(8, 4)):     \"\"\"     Creates and displays a bar chart that represents the distribution of a column divided into specific ranges in a DataFrame,     grouped by a target variable and visualizing the relative percentages in each group.      Parameters:     data (pandas DataFrame): The DataFrame containing the data.     column (str): The name of the column for which the range distribution will be created.     bins (int or sequence of scalars): The number of bins or bin edges for the division.     vo (str): The name of the target variable to group the data in the bar chart.     figsize (tuple, optional): The size of the bar chart figure. Default is (8, 4).      Returns:     None     \"\"\"      # Configure the Seaborn style     sns.set(style='whitegrid')      # Add a new column to the DataFrame with the ranges of the specific column     data[column + 'Range'] = pd.cut(data[column], bins=bins, right=False)      # Calculate the count of each group and restructure the data     counts = data.groupby([vo, column + 'Range']).size().reset_index(name='Count')      # Calculate the percentages by category     counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))      # Bar chart with Seaborn     plt.figure(figsize=figsize)     ax = sns.barplot(data=counts, x=column + 'Range', y='Percentage', hue=vo, palette='Greens', edgecolor='black')      # Rotate the x-axis labels (45 degrees) and add values on each bar (excluding 0%)     for p in ax.patches:         if p.get_height() != 0:  # If the value is not 0%             ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center',                         va='center', xytext=(0, 10), textcoords='offset points', fontsize=8)      # Set title and axis labels     plt.title(f'Range Distribution of column {column}')     plt.xlabel('Ranges')     plt.ylabel('Frequency')     plt.xticks(rotation=0)  # Rotate x-axis labels for better readability     plt.tight_layout()      # Display the chart     plt.show()      # Remove extra column     data.drop(column + 'Range', axis=1, inplace=True) In\u00a0[\u00a0]: Copied! <pre>def plot_barplot(data, column, figsize=(8, 4)):\n    \"\"\"\n    Creates and displays a bar chart that represents the distribution of a column in a DataFrame.\n\n    Parameters:\n    data (pandas DataFrame): The DataFrame containing the data.\n    column (str): The name of the column for which the bar chart will be created.\n    figsize (tuple, optional): The size of the bar chart figure. Default is (8, 4).\n\n    Returns:\n    None\n    \"\"\"\n\n    # Configure the Seaborn style\n    sns.set(style='whitegrid')\n\n    # Calculate the percentages of each category in the specified column\n    temp_percentages = (data[column].value_counts(normalize=True) * 100).sort_index()\n\n    # Create the bar chart with percentages\n    plt.figure(figsize=figsize)\n    temp_percentages.plot(kind='bar', color='#bce4b5', edgecolor='black')\n\n    # Add value annotations on the bars (percentages)\n    for i, value in enumerate(temp_percentages):\n        plt.text(i, value + 1, f'{value:.2f}%', ha='center', va='bottom', fontsize=9)\n\n    # Set title and axis labels\n    plt.title(f'Distribution of column {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n    plt.xticks(rotation=0)  # Rotate x-axis labels for better readability\n    plt.ylim(0, 100)  # Set y-axis range from 0 to 100 for percentages\n    plt.show()\n</pre> def plot_barplot(data, column, figsize=(8, 4)):     \"\"\"     Creates and displays a bar chart that represents the distribution of a column in a DataFrame.      Parameters:     data (pandas DataFrame): The DataFrame containing the data.     column (str): The name of the column for which the bar chart will be created.     figsize (tuple, optional): The size of the bar chart figure. Default is (8, 4).      Returns:     None     \"\"\"      # Configure the Seaborn style     sns.set(style='whitegrid')      # Calculate the percentages of each category in the specified column     temp_percentages = (data[column].value_counts(normalize=True) * 100).sort_index()      # Create the bar chart with percentages     plt.figure(figsize=figsize)     temp_percentages.plot(kind='bar', color='#bce4b5', edgecolor='black')      # Add value annotations on the bars (percentages)     for i, value in enumerate(temp_percentages):         plt.text(i, value + 1, f'{value:.2f}%', ha='center', va='bottom', fontsize=9)      # Set title and axis labels     plt.title(f'Distribution of column {column}')     plt.xlabel(column)     plt.ylabel('Frequency')     plt.xticks(rotation=0)  # Rotate x-axis labels for better readability     plt.ylim(0, 100)  # Set y-axis range from 0 to 100 for percentages     plt.show() In\u00a0[\u00a0]: Copied! <pre>def plot_barplot_vo(data, column, vo, figsize=(8, 4)):\n    \"\"\"\n    Creates and displays a bar chart that represents the distribution of a column divided by a target variable.\n\n    Parameters:\n    data (pandas DataFrame): The DataFrame containing the data.\n    column (str): The name of the column for which the bar chart will be created.\n    vo (str): The name of the target variable to group the data in the bar chart.\n    figsize (tuple, optional): The size of the bar chart figure. Default is (8, 4).\n\n    Returns:\n    None\n    \"\"\"\n\n    # Configure the Seaborn style\n    sns.set(style='whitegrid')\n\n    # Calculate the count of each group and restructure the data\n    counts = data.groupby([vo, column]).size().reset_index(name='Count')\n\n    # Calculate the percentages by category\n    counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))\n\n    # Bar chart with Seaborn\n    plt.figure(figsize=figsize)\n    ax = sns.barplot(data=counts, x=column, y='Percentage', hue=vo, palette='Greens', edgecolor='black')\n\n    # Rotate the x-axis labels (45 degrees) and add values on each bar (excluding 0%)\n    for p in ax.patches:\n        if p.get_height() != 0:  # If the value is not 0%\n            ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center',\n                        va='center', xytext=(0, 10), textcoords='offset points', fontsize=8)\n\n    # Set title and axis labels\n    plt.title(f'Range Distribution of column {column}')\n    plt.xlabel('Ranges')\n    plt.ylabel('Frequency')\n    plt.xticks(rotation=0)  # Rotate x-axis labels for better readability\n    plt.tight_layout()\n\n    # Display the chart\n    plt.show()\n</pre> def plot_barplot_vo(data, column, vo, figsize=(8, 4)):     \"\"\"     Creates and displays a bar chart that represents the distribution of a column divided by a target variable.      Parameters:     data (pandas DataFrame): The DataFrame containing the data.     column (str): The name of the column for which the bar chart will be created.     vo (str): The name of the target variable to group the data in the bar chart.     figsize (tuple, optional): The size of the bar chart figure. Default is (8, 4).      Returns:     None     \"\"\"      # Configure the Seaborn style     sns.set(style='whitegrid')      # Calculate the count of each group and restructure the data     counts = data.groupby([vo, column]).size().reset_index(name='Count')      # Calculate the percentages by category     counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))      # Bar chart with Seaborn     plt.figure(figsize=figsize)     ax = sns.barplot(data=counts, x=column, y='Percentage', hue=vo, palette='Greens', edgecolor='black')      # Rotate the x-axis labels (45 degrees) and add values on each bar (excluding 0%)     for p in ax.patches:         if p.get_height() != 0:  # If the value is not 0%             ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center',                         va='center', xytext=(0, 10), textcoords='offset points', fontsize=8)      # Set title and axis labels     plt.title(f'Range Distribution of column {column}')     plt.xlabel('Ranges')     plt.ylabel('Frequency')     plt.xticks(rotation=0)  # Rotate x-axis labels for better readability     plt.tight_layout()      # Display the chart     plt.show() In\u00a0[\u00a0]: Copied! <pre>def calculate_percentage_vo_int(data, column, vo):\n    \"\"\"\n    Calculates the relative percentages of each group divided by a target variable (vo) in a DataFrame,\n    keeping the column of interest as the index in the pivot table.\n\n    Parameters:\n    data (pandas DataFrame): The DataFrame containing the data.\n    column (str): The name of the column for which the percentages will be calculated.\n    vo (str): The name of the target variable to group the data and calculate the percentages.\n\n    Returns:\n    pandas DataFrame: A pivot table with the relative percentages.\n    \"\"\"\n\n    # Calculate the count of each group and restructure the data\n    counts = data.groupby([vo, column]).size().reset_index(name='Count')\n\n    # Calculate the percentages by category\n    counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))\n\n    # Create a pivot table with the percentages\n    pivot_counts = counts.pivot_table(values=['Count', 'Percentage'], index=column, columns=vo)\n\n    return pivot_counts\n</pre> def calculate_percentage_vo_int(data, column, vo):     \"\"\"     Calculates the relative percentages of each group divided by a target variable (vo) in a DataFrame,     keeping the column of interest as the index in the pivot table.      Parameters:     data (pandas DataFrame): The DataFrame containing the data.     column (str): The name of the column for which the percentages will be calculated.     vo (str): The name of the target variable to group the data and calculate the percentages.      Returns:     pandas DataFrame: A pivot table with the relative percentages.     \"\"\"      # Calculate the count of each group and restructure the data     counts = data.groupby([vo, column]).size().reset_index(name='Count')      # Calculate the percentages by category     counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))      # Create a pivot table with the percentages     pivot_counts = counts.pivot_table(values=['Count', 'Percentage'], index=column, columns=vo)      return pivot_counts In\u00a0[\u00a0]: Copied! <pre>def calculate_percentage_vo(data, column, bins, vo):\n    \"\"\"\n    Calculates the relative percentages of each group divided by a target variable (vo) in a DataFrame,\n    within specific ranges defined by column and bins.\n\n    Parameters:\n    data (pandas DataFrame): The DataFrame containing the data.\n    column (str): The name of the column for which the percentages will be calculated.\n    bins (int or sequence of scalars): The number of bins or bin edges for the division.\n    vo (str): The name of the target variable to group the data and calculate the percentages.\n\n    Returns:\n    pandas DataFrame: A pivot table with the relative percentages.\n    \"\"\"\n\n    # Add a new column to the DataFrame with the ranges of the specific column\n    data[column + 'Range'] = pd.cut(data[column], bins=bins, right=False)\n\n    # Calculate the count of each group and restructure the data\n    counts = data.groupby([vo, column + 'Range']).size().reset_index(name='Count')\n\n    # Calculate the percentages by category\n    counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))\n\n    # Create a pivot table with the percentages\n    pivot_counts = counts.pivot_table(values=['Count', 'Percentage'], index=column + 'Range', columns=vo)\n\n    # Remove extra column\n    data.drop(column + 'Range', axis=1, inplace=True)\n\n    return pivot_counts\n</pre> def calculate_percentage_vo(data, column, bins, vo):     \"\"\"     Calculates the relative percentages of each group divided by a target variable (vo) in a DataFrame,     within specific ranges defined by column and bins.      Parameters:     data (pandas DataFrame): The DataFrame containing the data.     column (str): The name of the column for which the percentages will be calculated.     bins (int or sequence of scalars): The number of bins or bin edges for the division.     vo (str): The name of the target variable to group the data and calculate the percentages.      Returns:     pandas DataFrame: A pivot table with the relative percentages.     \"\"\"      # Add a new column to the DataFrame with the ranges of the specific column     data[column + 'Range'] = pd.cut(data[column], bins=bins, right=False)      # Calculate the count of each group and restructure the data     counts = data.groupby([vo, column + 'Range']).size().reset_index(name='Count')      # Calculate the percentages by category     counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))      # Create a pivot table with the percentages     pivot_counts = counts.pivot_table(values=['Count', 'Percentage'], index=column + 'Range', columns=vo)      # Remove extra column     data.drop(column + 'Range', axis=1, inplace=True)      return pivot_counts In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}]}